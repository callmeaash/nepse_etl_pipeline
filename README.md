# ğŸ“Š NEPSE ETL Pipeline with Airflow

This project implements an **ETL (Extract, Transform, Load) pipeline** using **Apache Airflow** for orchestration.

The pipeline extracts stock data from an API, transforms it into a structured format, and loads it into a local database as well as csv file.

---

## ğŸš€ Features
- **Extract**: Fetch stock OHLC data from an API.
- **Transform**: Clean, validate, and process raw data.
- **Load**: Store processed data into database and csv file.
- **Airflow DAG**: Automates and schedules the ETL workflow.
- **Logging**: Logs ETL pipeline steps for monitoring.

---

## ğŸ› ï¸ Tools Used

- **[Pandas](https://pandas.pydata.org/)** â€“ For data extraction and transformation.
- **[SQLAlchemy](https://www.sqlalchemy.org/)** â€“ To interact with databases in a scalable and database-agnostic way.
- **[Apache Airflow](https://airflow.apache.org/)** â€“ For orchestrating, scheduling, and monitoring ETL workflows.
- **Logging (Python `logging` module)** â€“ For structured and configurable logging of pipeline events and errors.
- **[Pytest](https://docs.pytest.org/)** â€“ For writing and running unit tests to ensure pipeline reliability.


---

## ğŸ—‚ï¸ Project Structure
```bash
etl-project/
â”‚â”€â”€ dags/                    # Airflow DAG definitions
â”‚   â””â”€â”€ etl_pipeline_dag.py           # Main ETL DAG
â”‚
|â”€â”€ database_schema/
|   â””â”€â”€ erdiagram            # ER diagram of database
â”‚   â””â”€â”€ sqliteSchema.sql
â”‚   â””â”€â”€ schema.sql           # PostgreSQL database schema  
â”‚
â”‚â”€â”€ stock_names_data/        # File storing stocks symbol
â”‚   â””â”€â”€ stocks.csv      
â”‚
â”‚â”€â”€ pipeline_utils/          # ETL logic
â”‚   â”œâ”€â”€ extract.py           # Data extraction functions
â”‚   â”œâ”€â”€ transform.py         # Data transformation functions
â”‚   â”œâ”€â”€ load.py              # Data loading functions
â”‚   â””â”€â”€ __init__.py
â”‚
â”‚â”€â”€ tests/                   # Unit tests
â”‚   â””â”€â”€ test_extract.py
â”‚   â””â”€â”€ test_transform.py
â”‚   â””â”€â”€ test_load.py
â”‚
|â”€â”€ docker-compose-yaml      # Airflow configs for docker
â”‚â”€â”€ etl_pipeline.py          # Python script to execute the pipeline
â”‚â”€â”€ stocks_ohlc.db           # SQLite database (created after first run)
â”‚â”€â”€ requirements.txt         # Python dependencies
â”‚â”€â”€ README.md                # Project documentation

```

## âš™ï¸ Setup and Installation

### 1. Clone the Repository
```bash
git clone https://github.com/callmeaash/nepse_etl_pipeline.git
cd nepse_etl_pipeline
```

### 2. Create Virtual Environment
```bash
python -m venv .venv
source .venv/bin/activate    # On Linux/Mac
.venv\Scripts\activate     # On Windows
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Setup Airflow with Docker
Make sure you have **Docker** and **Docker Compose** installed.



#### Start Airflow
```bash
docker-compose up -d
```


### 5. Running the ETL Script Manually (Optional)
```bash
python etl_pipeline.py
```

### 6. Logs (For Manual ETL execution)
Check logs in:
```bash
etl_pipeline.log
```

---

## ğŸ§ª Testing
Run unit tests:
```bash
pytest tests/
```

---

## ğŸ“ Notes
- Environment variables must be defined in `.env`  
- SQLite DB (`stocks_ohlc.db`) is used for local development.  
- For production, consider PostgreSQL or MySQL.
- Extract more stocks symbol using the scrape function available in scraping_utils directory and append them in the stocks.csv in stock_names_data directory
